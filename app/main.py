import os
import sys
import glob
from typing import List, Annotated, TypedDict, Literal
from contextlib import asynccontextmanager
import re

# FastAPI
from fastapi import FastAPI
from pydantic import BaseModel, Field
import uvicorn

# LangChain & Models
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool  # [NEW] Tool ë°ì½”ë ˆì´í„°

# LangGraph
from langgraph.graph import END, StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode, tools_condition  # [NEW] ë„êµ¬ ë…¸ë“œ

# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# [NEW] OPC í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from opc_client import IgnitionOpcClient

# --- [0. ì„¤ì •] ---
load_dotenv()

EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
DB_PATH = "./faiss_index"
LLM_MODEL_NAME = "qwen2.5:7b"

# Ignition OPC UA ì£¼ì†Œ (ê¸°ë³¸ê°’)
OPC_ENDPOINT = os.getenv("OPC_ENDPOINT", "opc.tcp://localhost:62541")
OPC_USER = os.getenv("OPC_USER", "Admin")
OPC_PASSWORD = os.getenv("OPC_PASSWORD", "P@ssw0rd")

opc_client = IgnitionOpcClient(OPC_ENDPOINT)

# --- [1. ë„êµ¬(Tools) ì •ì˜] ---


@tool
async def read_ignition_tag(tag_path: str):
    """
    Ignition SCADAì˜ íƒœê·¸ ê°’ì„ ì½ìŠµë‹ˆë‹¤.
    'í˜„ì¬ ì˜¨ë„ ì•Œë ¤ì¤˜', 'ìƒíƒœ í™•ì¸í•´ì¤˜' ê°™ì€ ì§ˆë¬¸ì— ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        tag_path: ì½ì„ íƒœê·¸ì˜ ì „ì²´ ê²½ë¡œ (ì˜ˆ: '[default]Tank/Temperature')
    """
    print(f"ğŸ› ï¸ [Tool] íƒœê·¸ ì½ê¸° ì‹œë„: {tag_path}")
    return await opc_client.read_tag(tag_path)


@tool
async def write_ignition_tag(tag_path: str, value: str):
    """
    Ignition SCADAì˜ íƒœê·¸ì— ê°’ì„ ì”ë‹ˆë‹¤(ì œì–´).
    'ì„¤ì •ê°’ì„ 50ìœ¼ë¡œ ë°”ê¿”', 'ëª¨í„° ì¼œ' ê°™ì€ ëª…ë ¹ì— ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        tag_path: ì“¸ íƒœê·¸ì˜ ì „ì²´ ê²½ë¡œ (ì˜ˆ: '[default]Tank/Setpoint')
        value: ë³€ê²½í•  ê°’ (ìˆ«ìë‚˜ ë¬¸ìì—´)
    """
    print(f"ğŸ› ï¸ [Tool] íƒœê·¸ ì“°ê¸° ì‹œë„: {tag_path} -> {value}")
    return await opc_client.write_tag(tag_path, value)


# ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡
tools = [read_ignition_tag, write_ignition_tag]


# --- [2. Lifespan & Setup] ---
def langsmith_setup(project_name="Ignition-Agent-RAG"):
    if os.environ.get("LANGCHAIN_API_KEY"):
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
        os.environ["LANGCHAIN_PROJECT"] = project_name
        print(f"[System] LangSmith ì¶”ì  í™œì„±í™”: {project_name}")


langsmith_setup()


@asynccontextmanager
async def lifespan(app: FastAPI):
    global global_retriever
    print("\n[System] ì„œë²„ ì´ˆê¸°í™” ì¤‘...")

    print(f"[System] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (CUDA)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )

    if os.path.exists(DB_PATH):
        print("[System] ë²¡í„° DB ë¡œë”© ì¤‘...")
        try:
            vectorstore = FAISS.load_local(
                DB_PATH, embeddings, allow_dangerous_deserialization=True
            )
            global_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            print("[System] DB ë¡œë”© ì™„ë£Œ.")
        except Exception as e:
            print(f"[Error] DB ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        print("[System] âš ï¸ ì €ì¥ëœ DBê°€ ì—†ìŠµë‹ˆë‹¤. (ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”)")

    yield
    print("[System] ì„œë²„ ì¢…ë£Œ")


app = FastAPI(title="Ignition RAG Agent", lifespan=lifespan)


# --- [3. LangGraph ë¡œì§] ---


class GradeDocuments(BaseModel):
    binary_score: str = Field(description="'yes' or 'no'")


class GraphState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Document]
    force_tool: bool
    forced_tag_path: str


# 1. ë¬¸ì„œ ê²€ìƒ‰
def retrieve(state: GraphState):
    print("\n[1] ë¬¸ì„œ ê²€ìƒ‰")
    question = state["messages"][-1].content
    if global_retriever is None:
        return {"documents": []}

    docs = global_retriever.invoke(question)
    print(f" -> {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
    return {"documents": docs}


# 2. ë¬¸ì„œ í‰ê°€
def grade_documents(state: GraphState):
    print("\n[2] ë¬¸ì„œ í‰ê°€")
    question = state["messages"][-1].content
    documents = state["documents"]

    llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0, num_gpu=-1)
    parser = JsonOutputParser(pydantic_object=GradeDocuments)

    # í‰ê°€ í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a grader. Return JSON {{'binary_score': 'yes'}} if the document is relevant to the question, otherwise {{'binary_score': 'no'}}.",
            ),
            ("human", "Doc: {document}\nQuestion: {question}"),
        ]
    )
    chain = prompt | llm | parser

    filtered_docs = []
    for doc in documents:
        try:
            score = chain.invoke({"question": question, "document": doc.page_content})
            if score.get("binary_score") == "yes":
                filtered_docs.append(doc)
        except:
            continue

    print(f" -> {len(filtered_docs)}ê°œ ë¬¸ì„œ ìœ íš¨í•¨")
    return {"documents": filtered_docs}


CMD_WORDS = ["ì¼œ", "êº¼", "ë©ˆ", "ì •ì§€", "ì‹œì‘", "ê°€ë™", "on", "off", "ì„¤ì •", "set"]
DEVICE_HINT = re.compile(r"\bFAN\d+\b", re.IGNORECASE)  # FAN1, fan2 ê°™ì€ íŒ¨í„´

TAG_PATTERN = re.compile(r"(\[[^\]]+\][A-Za-z0-9_\-\/]+)")


# ë„êµ¬ ì‚¬ìš© ê°•ì œ ì—¬ë¶€ íŒë‹¨
def detect_realtime_intent(state: GraphState):
    text = state["messages"][-1].content
    lowered = text.lower()

    # 1) ì‚¬ìš©ìê°€ íƒœê·¸ë¥¼ ì§ì ‘ ì“´ ê²½ìš°
    m = TAG_PATTERN.search(text)
    tag = m.group(1) if m else ""

    # 2) ì œì–´ ëª…ë ¹ì¸ì§€ íŒë‹¨
    is_cmd = any(w in lowered for w in CMD_WORDS)

    # 3) ì¥ë¹„ íŒíŠ¸(ì˜ˆ: FAN1)ë¼ë„ ìˆìœ¼ë©´ ì œì–´ë¡œ ì·¨ê¸‰
    has_device = bool(DEVICE_HINT.search(text))

    force = is_cmd and (bool(tag) or has_device)

    return {"force_tool": force, "forced_tag_path": tag}


# ë„êµ¬ ì‚¬ìš© ê°•ì œ ì—¬ë¶€ íŒë‹¨
def detect_realtime_intent(state: GraphState):
    text = state["messages"][-1].content
    has_tag = "[default]" in text

    forced = ""
    if has_tag:
        m = re.search(r"(\[default\][\w\/\-]+)", text)
        forced = m.group(1) if m else ""

    return {"force_tool": has_tag, "forced_tag_path": forced}


async def force_control(state: GraphState):
    text = state["messages"][-1].content.lower()

    # íƒœê·¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì“°ê³ 
    tag = state.get("forced_tag_path") or ""

    # ì—†ìœ¼ë©´ â€œê·œì¹™ ê¸°ë°˜ ë§¤í•‘â€ìœ¼ë¡œ ê²°ì • (FAN1 â†’ [default]FAN1/Status)
    if not tag and DEVICE_HINT.search(state["messages"][-1].content):
        dev = DEVICE_HINT.search(state["messages"][-1].content).group(0).upper()
        tag = f"[default]{dev}/Status"

    # ê°’ ê²°ì •
    value = "OFF" if ("ë©ˆ" in text or "ì •ì§€" in text or "off" in text) else "ON"

    result = await opc_client.write_tag(tag, value)
    return {"messages": [AIMessage(content=f"[ì œì–´ ì‹¤í–‰]\n{result}")]}


# 3. RAG ë‹µë³€ (ë¬¸ì„œ ê¸°ë°˜)
def generate_rag(state: GraphState):
    print("\n[3-A] RAG ë‹µë³€ ìƒì„±")
    documents = state["documents"]
    question = state["messages"][-1].content

    llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0, num_gpu=-1, num_ctx=4096)

    system_prompt = (
        "You are a Data Center Expert. "
        "Answer the question strictly in **Korean**, based **only** on the provided [Context]. "
        "Do not fabricate information."
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "Context:\n{context}\n\nQuestion:\n{question}"),
        ]
    )
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"context": documents, "question": question})
    return {"messages": [AIMessage(content=response)]}


# 4. ì¼ë°˜ ëŒ€í™” ë° ë„êµ¬ ì‚¬ìš© (ë¬¸ì„œ ì—†ìŒ)
def generate_chat(state: GraphState):
    print("\n[3-B] ì¼ë°˜ ëŒ€í™”/ë„êµ¬ ëª¨ë“œ")
    messages = state["messages"]

    # ë„êµ¬ ë°”ì¸ë”© (Bind Tools)
    llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0.1, num_gpu=-1)
    llm_with_tools = llm.bind_tools(tools)

    system_msg = SystemMessage(
        content=(
            "You are an Ignition SCADA Operator. "
            "NEVER answer tag current values from memory. "
            "If the user asks for any current/now/real-time value or status, you MUST call read_ignition_tag again. "
            "If the user wants to change values, use write_ignition_tag. "
            "Answer naturally in Korean."
        )
    )

    # LLM í˜¸ì¶œ (ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ ê²°ì • í¬í•¨)
    response = llm_with_tools.invoke([system_msg] + messages)
    return {"messages": [response]}


# 5. ë¼ìš°íŒ… ê²°ì •
def route_after_detect(state: GraphState):
    if state["documents"]:
        return "generate_rag"
    if state.get("force_tool"):
        return "force_control"
    return "generate_chat"


# --- [4. ê·¸ë˜í”„ êµ¬ì¶•] ---#
def build_graph():
    memory = MemorySaver()
    workflow = StateGraph(GraphState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("generate_rag", generate_rag)
    workflow.add_node("generate_chat", generate_chat)
    workflow.add_node("detect_realtime_intent", detect_realtime_intent)
    workflow.add_node("force_control", force_control)
    workflow.add_node("tools", ToolNode(tools))

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "retrieve")
    workflow.add_edge("retrieve", "grade_documents")
    workflow.add_edge("grade_documents", "detect_realtime_intent")

    # detect ê²°ê³¼ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "detect_realtime_intent",
        route_after_detect,
        {
            "generate_rag": "generate_rag",
            "generate_chat": "generate_chat",
            "force_control": "force_control",
        },
    )

    # force_controlëŠ” ì¢…ë£Œ
    workflow.add_edge("force_control", END)

    # generate_chatì—ì„œ tool_callì´ ìˆìœ¼ë©´ toolsë¡œ
    workflow.add_conditional_edges(
        "generate_chat",
        tools_condition,
        {"tools": "tools", END: END},
    )

    # tools ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ generate_chatë¡œ ë³´ë‚´ ìµœì¢… ë¬¸ì¥ ìƒì„±
    workflow.add_edge("tools", "generate_chat")

    # RAGëŠ” ì¢…ë£Œ
    workflow.add_edge("generate_rag", END)

    return workflow.compile(checkpointer=memory)


app_graph = build_graph()


# --- [5. API Endpoint] ---
class QueryRequest(BaseModel):
    question: str
    thread_id: str = "default_user"


@app.post("/ask")
async def ask_rag(request: QueryRequest):
    print(f"\n[Request] Thread: {request.thread_id} | Q: {request.question}")

    # Memory ì„¤ì •ì„ ìœ„í•œ config
    config = RunnableConfig(configurable={"thread_id": request.thread_id})

    # ì´ˆê¸° ì…ë ¥ ë©”ì‹œì§€
    inputs = {"messages": [HumanMessage(content=request.question)]}

    # [ìˆ˜ì •ë¨] ë¹„ë™ê¸° ë„êµ¬(OPC UA)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ await ainvoke()ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.
    result = await app_graph.ainvoke(inputs, config=config)

    # ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ë§ˆì§€ë§‰ ë©”ì‹œì§€)
    final_answer = result["messages"][-1].content

    # ì†ŒìŠ¤ ì •ë¦¬ (RAG ëª¨ë“œì¼ ë•Œë§Œ ì¡´ì¬)
    sources = []
    if "documents" in result and result["documents"]:
        sources = list(
            set([doc.metadata.get("source", "Unknown") for doc in result["documents"]])
        )

    print(f"[Response] ì™„ë£Œ (Sources: {len(sources)})")

    return {
        "question": request.question,
        "answer": final_answer,
        "sources": sources,
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
